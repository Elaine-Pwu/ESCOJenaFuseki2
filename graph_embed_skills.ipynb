{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177629ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "import torch\n",
    "\n",
    "print(\"Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23542f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "503fce4c",
   "metadata": {},
   "source": [
    "<!-- ## How to Extract Skill Subgraph from Raw Data\n",
    "\n",
    "This cell demonstrates how to extract a subgraph containing skills from raw RDF data. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc2425",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1468b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading triples from out_esco/skill_subgraph_from_rdf.tsv...\n",
      "Loaded 1,816,462 triples\n",
      "\n",
      "First few triples:\n",
      "                                             subject  \\\n",
      "0  http://data.europa.eu/esco/skill/0d168770-4d9c...   \n",
      "1  http://data.europa.eu/esco/skill/918459f1-147d...   \n",
      "2  http://data.europa.eu/esco/relation/56091B30-1...   \n",
      "3  http://data.europa.eu/esco/skill/a549dcdf-3771...   \n",
      "4  http://data.europa.eu/esco/skill/f66cbeb5-2a8e...   \n",
      "\n",
      "                                           predicate  \\\n",
      "0  http://data.europa.eu/esco/model#isOptionalSki...   \n",
      "1  http://www.w3.org/2004/02/skos/core#broaderTra...   \n",
      "2  http://data.europa.eu/esco/model#isAssociationFor   \n",
      "3   http://www.w3.org/2004/02/skos/core#topConceptOf   \n",
      "4               http://purl.org/dc/terms/description   \n",
      "\n",
      "                                              object  \n",
      "0  http://data.europa.eu/esco/occupation/f2b15a0e...  \n",
      "1  http://data.europa.eu/esco/skill/496932f1-0b6b...  \n",
      "2  http://data.europa.eu/esco/skill/c241f5a5-e23f...  \n",
      "3  http://data.europa.eu/esco/concept-scheme/memb...  \n",
      "4  http://data.europa.eu/esco/node-literal/eb2bce...  \n",
      "\n",
      "Unique subjects: 138,805\n",
      "Unique predicates: 25\n",
      "Unique objects: 1,178,506\n",
      "\n",
      "Identifying skill URIs using prefix: http://data.europa.eu/esco/skill/\n",
      "Method: Check if URI string starts with this prefix\n",
      "\n",
      "URI Identification Summary:\n",
      "  - Prefix pattern: 'http://data.europa.eu/esco/skill/'\n",
      "  - Identification method: String prefix matching (str.startswith())\n",
      "  - Why this works: ESCO uses consistent namespace for all skill entities\n",
      "\n",
      "✓ Extracted 15,163 unique skill URIs from triples\n",
      "  From subjects: 15,163\n",
      "  From objects: 15,100\n",
      "\n",
      "First few skill URIs:\n",
      "  http://data.europa.eu/esco/skill/0005c151-5b5a-4a66-8aac-60e734beb1ab\n",
      "  http://data.europa.eu/esco/skill/00064735-8fad-454b-90c7-ed858cc993f2\n",
      "  http://data.europa.eu/esco/skill/000709ed-2be5-4193-b056-45a97698d828\n",
      "  http://data.europa.eu/esco/skill/0007bdc2-dd15-4824-b7d6-416522c46f35\n",
      "  http://data.europa.eu/esco/skill/00090cc1-1f27-439e-a4e0-19a87a501bfc\n"
     ]
    }
   ],
   "source": [
    "# Load knowledge graph triples\n",
    "triples_file = \"out_esco/skill_subgraph_from_rdf.tsv\"\n",
    "print(f\"Loading triples from {triples_file}...\")\n",
    "\n",
    "# Read triples (tab-separated: subject, predicate, object)\n",
    "triples_df = pd.read_csv(\n",
    "    triples_file, \n",
    "    sep=\"\\t\", \n",
    "    header=None, \n",
    "    names=[\"subject\", \"predicate\", \"object\"],\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(triples_df):,} triples\")\n",
    "print(\"\\nFirst few triples:\")\n",
    "print(triples_df.head())\n",
    "print(f\"\\nUnique subjects: {triples_df['subject'].nunique():,}\")\n",
    "print(f\"Unique predicates: {triples_df['predicate'].nunique():,}\")\n",
    "print(f\"Unique objects: {triples_df['object'].nunique():,}\")\n",
    "\n",
    "# Extract skill URIs from triples (URIs that start with skill namespace)\n",
    "skill_uri_prefix = \"http://data.europa.eu/esco/skill/\"\n",
    "print(f\"\\nIdentifying skill URIs using prefix: {skill_uri_prefix}\")\n",
    "print(\"Method: Check if URI string starts with this prefix\")\n",
    "\n",
    "all_skill_uris = set()\n",
    "\n",
    "# Get skill URIs from subjects\n",
    "skill_uris_from_subjects = set(triples_df[triples_df['subject'].str.startswith(skill_uri_prefix)]['subject'].unique())\n",
    "all_skill_uris.update(skill_uris_from_subjects)\n",
    "\n",
    "# Get skill URIs from objects (in case some skills appear as objects)\n",
    "skill_uris_from_objects = set(triples_df[triples_df['object'].str.startswith(skill_uri_prefix)]['object'].unique())\n",
    "all_skill_uris.update(skill_uris_from_objects)\n",
    "\n",
    "skill_uris_list = sorted(list(all_skill_uris))\n",
    "\n",
    "print(f\"\\nURI Identification Summary:\")\n",
    "print(f\"  - Prefix pattern: '{skill_uri_prefix}'\")\n",
    "print(f\"  - Identification method: String prefix matching (str.startswith())\")\n",
    "print(f\"  - Why this works: ESCO uses consistent namespace for all skill entities\")\n",
    "print(f\"\\n✓ Extracted {len(skill_uris_list):,} unique skill URIs from triples\")\n",
    "print(f\"  From subjects: {len(skill_uris_from_subjects):,}\")\n",
    "print(f\"  From objects: {len(skill_uris_from_objects):,}\")\n",
    "print(f\"\\nFirst few skill URIs:\")\n",
    "for uri in skill_uris_list[:5]:\n",
    "    print(f\"  {uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8302cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "All Predicates in the Dataset\n",
      "======================================================================\n",
      "\n",
      "Total unique predicates: 25\n",
      "Total triples: 1,816,462\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Predicate List (sorted by frequency):\n",
      "----------------------------------------------------------------------\n",
      " 1. http://www.w3.org/2008/05/skos-xl#prefLabel\n",
      "    Count: 423,633 triples (23.32%)\n",
      " 2. http://purl.org/dc/terms/description\n",
      "    Count: 396,455 triples (21.83%)\n",
      " 3. http://www.w3.org/2008/05/skos-xl#altLabel\n",
      "    Count: 203,535 triples (11.21%)\n",
      " 4. http://data.europa.eu/esco/model#hasAssociation\n",
      "    Count: 125,504 triples (6.91%)\n",
      " 5. http://data.europa.eu/esco/model#target\n",
      "    Count: 120,374 triples (6.63%)\n",
      " 6. http://www.w3.org/2004/02/skos/core#broaderTransitive\n",
      "    Count: 83,586 triples (4.60%)\n",
      " 7. http://data.europa.eu/esco/model#relatedEssentialSkill\n",
      "    Count: 67,811 triples (3.73%)\n",
      " 8. http://data.europa.eu/esco/model#isEssentialSkillFor\n",
      "    Count: 67,811 triples (3.73%)\n",
      " 9. http://data.europa.eu/esco/model#isOptionalSkillFor\n",
      "    Count: 67,011 triples (3.69%)\n",
      "10. http://data.europa.eu/esco/model#relatedOptionalSkill\n",
      "    Count: 67,011 triples (3.69%)\n",
      "11. http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "    Count: 43,677 triples (2.40%)\n",
      "12. http://www.w3.org/2004/02/skos/core#inScheme\n",
      "    Count: 32,826 triples (1.81%)\n",
      "13. http://www.w3.org/2004/02/skos/core#broader\n",
      "    Count: 20,614 triples (1.13%)\n",
      "14. http://www.w3.org/2004/02/skos/core#narrower\n",
      "    Count: 20,614 triples (1.13%)\n",
      "15. http://www.w3.org/2008/05/skos-xl#hiddenLabel\n",
      "    Count: 17,025 triples (0.94%)\n",
      "16. http://data.europa.eu/esco/model#skillReuseLevel\n",
      "    Count: 14,251 triples (0.78%)\n",
      "17. http://data.europa.eu/esco/model#skillType\n",
      "    Count: 14,251 triples (0.78%)\n",
      "18. http://www.w3.org/2004/02/skos/core#hasTopConcept\n",
      "    Count: 11,131 triples (0.61%)\n",
      "19. http://www.w3.org/2004/02/skos/core#topConceptOf\n",
      "    Count: 11,131 triples (0.61%)\n",
      "20. http://data.europa.eu/esco/model#isAssociationFor\n",
      "    Count: 5,971 triples (0.33%)\n",
      "21. http://www.w3.org/2004/02/skos/core#scopeNote\n",
      "    Count: 937 triples (0.05%)\n",
      "22. http://purl.org/dc/terms/isReplacedBy\n",
      "    Count: 439 triples (0.02%)\n",
      "23. http://purl.org/dc/terms/replaces\n",
      "    Count: 439 triples (0.02%)\n",
      "24. http://www.w3.org/ns/adms#identifier\n",
      "    Count: 420 triples (0.02%)\n",
      "25. http://www.w3.org/2004/02/skos/core#definition\n",
      "    Count: 5 triples (0.00%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Predicate List (for copy-paste, sorted alphabetically):\n",
      "----------------------------------------------------------------------\n",
      " 1. http://data.europa.eu/esco/model#hasAssociation\n",
      " 2. http://data.europa.eu/esco/model#isAssociationFor\n",
      " 3. http://data.europa.eu/esco/model#isEssentialSkillFor\n",
      " 4. http://data.europa.eu/esco/model#isOptionalSkillFor\n",
      " 5. http://data.europa.eu/esco/model#relatedEssentialSkill\n",
      " 6. http://data.europa.eu/esco/model#relatedOptionalSkill\n",
      " 7. http://data.europa.eu/esco/model#skillReuseLevel\n",
      " 8. http://data.europa.eu/esco/model#skillType\n",
      " 9. http://data.europa.eu/esco/model#target\n",
      "10. http://purl.org/dc/terms/description\n",
      "11. http://purl.org/dc/terms/isReplacedBy\n",
      "12. http://purl.org/dc/terms/replaces\n",
      "13. http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "14. http://www.w3.org/2004/02/skos/core#broader\n",
      "15. http://www.w3.org/2004/02/skos/core#broaderTransitive\n",
      "16. http://www.w3.org/2004/02/skos/core#definition\n",
      "17. http://www.w3.org/2004/02/skos/core#hasTopConcept\n",
      "18. http://www.w3.org/2004/02/skos/core#inScheme\n",
      "19. http://www.w3.org/2004/02/skos/core#narrower\n",
      "20. http://www.w3.org/2004/02/skos/core#scopeNote\n",
      "21. http://www.w3.org/2004/02/skos/core#topConceptOf\n",
      "22. http://www.w3.org/2008/05/skos-xl#altLabel\n",
      "23. http://www.w3.org/2008/05/skos-xl#hiddenLabel\n",
      "24. http://www.w3.org/2008/05/skos-xl#prefLabel\n",
      "25. http://www.w3.org/ns/adms#identifier\n",
      "\n",
      "======================================================================\n",
      "Next Step:\n",
      "======================================================================\n",
      "Please provide a blacklist of predicates to remove.\n",
      "Then run the next cell to filter the triples.\n"
     ]
    }
   ],
   "source": [
    "## List All Predicates\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"All Predicates in the Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count all predicates\n",
    "predicate_counts = triples_df['predicate'].value_counts()\n",
    "\n",
    "print(f\"\\nTotal unique predicates: {len(predicate_counts)}\")\n",
    "print(f\"Total triples: {len(triples_df):,}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Predicate List (sorted by frequency):\")\n",
    "print(\"-\" * 70)\n",
    "for idx, (predicate, count) in enumerate(predicate_counts.items(), 1):\n",
    "    print(f\"{idx:2d}. {predicate}\")\n",
    "    print(f\"    Count: {count:,} triples ({count/len(triples_df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Predicate List (for copy-paste, sorted alphabetically):\")\n",
    "print(\"-\" * 70)\n",
    "predicates_sorted = sorted(predicate_counts.index)\n",
    "for idx, predicate in enumerate(predicates_sorted, 1):\n",
    "    print(f\"{idx:2d}. {predicate}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Next Step:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Please provide a blacklist of predicates to remove.\")\n",
    "print(\"Then run the next cell to filter the triples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7d10ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Filter Triples by Predicate Blacklist\n",
      "======================================================================\n",
      "\n",
      "Blacklist contains 5 predicates:\n",
      "  1. http://data.europa.eu/esco/model#isEssentialSkillFor\n",
      "     Will remove 67,811 triples\n",
      "  2. http://data.europa.eu/esco/model#isOptionalSkillFor\n",
      "     Will remove 67,011 triples\n",
      "  3. http://www.w3.org/2004/02/skos/core#broader\n",
      "     Will remove 20,614 triples\n",
      "  4. http://www.w3.org/2004/02/skos/core#topConceptOf\n",
      "     Will remove 11,131 triples\n",
      "  5. http://purl.org/dc/terms/isReplacedBy\n",
      "     Will remove 439 triples\n",
      "\n",
      "======================================================================\n",
      "Filtering Triples\n",
      "======================================================================\n",
      "\n",
      "Original triples: 1,816,462\n",
      "Filtered triples: 1,649,456\n",
      "Removed triples: 167,006\n",
      "Removed percentage: 9.19%\n",
      "\n",
      "Original unique predicates: 25\n",
      "Remaining unique predicates: 20\n",
      "\n",
      "======================================================================\n",
      "Remaining Predicates:\n",
      "======================================================================\n",
      " 1. http://www.w3.org/2008/05/skos-xl#prefLabel: 423,633 triples\n",
      " 2. http://purl.org/dc/terms/description: 396,455 triples\n",
      " 3. http://www.w3.org/2008/05/skos-xl#altLabel: 203,535 triples\n",
      " 4. http://data.europa.eu/esco/model#hasAssociation: 125,504 triples\n",
      " 5. http://data.europa.eu/esco/model#target: 120,374 triples\n",
      " 6. http://www.w3.org/2004/02/skos/core#broaderTransitive: 83,586 triples\n",
      " 7. http://data.europa.eu/esco/model#relatedEssentialSkill: 67,811 triples\n",
      " 8. http://data.europa.eu/esco/model#relatedOptionalSkill: 67,011 triples\n",
      " 9. http://www.w3.org/1999/02/22-rdf-syntax-ns#type: 43,677 triples\n",
      "10. http://www.w3.org/2004/02/skos/core#inScheme: 32,826 triples\n",
      "11. http://www.w3.org/2004/02/skos/core#narrower: 20,614 triples\n",
      "12. http://www.w3.org/2008/05/skos-xl#hiddenLabel: 17,025 triples\n",
      "13. http://data.europa.eu/esco/model#skillType: 14,251 triples\n",
      "14. http://data.europa.eu/esco/model#skillReuseLevel: 14,251 triples\n",
      "15. http://www.w3.org/2004/02/skos/core#hasTopConcept: 11,131 triples\n",
      "16. http://data.europa.eu/esco/model#isAssociationFor: 5,971 triples\n",
      "17. http://www.w3.org/2004/02/skos/core#scopeNote: 937 triples\n",
      "18. http://purl.org/dc/terms/replaces: 439 triples\n",
      "19. http://www.w3.org/ns/adms#identifier: 420 triples\n",
      "20. http://www.w3.org/2004/02/skos/core#definition: 5 triples\n",
      "\n",
      "======================================================================\n",
      "Ready for Training\n",
      "======================================================================\n",
      "✓ Filtered triples are ready in 'triples_df_filtered'\n",
      "  Use 'triples_df_filtered' instead of 'triples_df' for training\n"
     ]
    }
   ],
   "source": [
    "## Filter Triples by Predicate Blacklist\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Filter Triples by Predicate Blacklist\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================\n",
    "# Enter the predicate blacklist to remove here\n",
    "# ============================================\n",
    "# Add the predicates you want to remove to the list below\n",
    "predicate_blacklist = [\n",
    "    \"http://data.europa.eu/esco/model#isEssentialSkillFor\",\n",
    "    \"http://data.europa.eu/esco/model#isOptionalSkillFor\",\n",
    "    \"http://www.w3.org/2004/02/skos/core#broader\",\n",
    "    \"http://www.w3.org/2004/02/skos/core#topConceptOf\",\n",
    "    \"http://purl.org/dc/terms/isReplacedBy\"\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\nBlacklist contains {len(predicate_blacklist)} predicates:\")\n",
    "if predicate_blacklist:\n",
    "    for idx, pred in enumerate(predicate_blacklist, 1):\n",
    "        count = len(triples_df[triples_df['predicate'] == pred])\n",
    "        print(f\"  {idx}. {pred}\")\n",
    "        print(f\"     Will remove {count:,} triples\")\n",
    "else:\n",
    "    print(\"  (Blacklist is empty - no predicates will be removed)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Filtering Triples\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Filter out predicates in the blacklist\n",
    "triples_df_filtered = triples_df[~triples_df['predicate'].isin(predicate_blacklist)].copy()\n",
    "\n",
    "print(f\"\\nOriginal triples: {len(triples_df):,}\")\n",
    "print(f\"Filtered triples: {len(triples_df_filtered):,}\")\n",
    "print(f\"Removed triples: {len(triples_df) - len(triples_df_filtered):,}\")\n",
    "\n",
    "if len(triples_df) - len(triples_df_filtered) > 0:\n",
    "    print(f\"Removed percentage: {(len(triples_df) - len(triples_df_filtered))/len(triples_df)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nOriginal unique predicates: {triples_df['predicate'].nunique()}\")\n",
    "print(f\"Remaining unique predicates: {triples_df_filtered['predicate'].nunique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Remaining Predicates:\")\n",
    "print(\"=\" * 70)\n",
    "remaining_predicates = triples_df_filtered['predicate'].value_counts()\n",
    "for idx, (predicate, count) in enumerate(remaining_predicates.items(), 1):\n",
    "    print(f\"{idx:2d}. {predicate}: {count:,} triples\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Ready for Training\")\n",
    "print(\"=\" * 70)\n",
    "print(\"✓ Filtered triples are ready in 'triples_df_filtered'\")\n",
    "print(\"  Use 'triples_df_filtered' instead of 'triples_df' for training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33264d",
   "metadata": {},
   "source": [
    "## Modifying data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bfbd6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TriplesFactory...\n",
      "\n",
      "How TriplesFactory works:\n",
      "1. Input: triples with subject, predicate, object (all are URI strings)\n",
      "2. TriplesFactory automatically:\n",
      "   - Extracts all unique entities from subject and object columns\n",
      "   - Assigns each unique entity a unique integer ID (0, 1, 2, ...)\n",
      "   - Preserves the original URI string as the 'label'\n",
      "   - Creates mapping: entity_id_to_label = {entity_id: original_uri_string}\n",
      "3. The 'label' IS the original URI string from the triples!\n",
      "\n",
      "✓ TriplesFactory created\n",
      "  Number of entities: 1,179,402\n",
      "  Number of relations: 40\n",
      "  Number of triples: 1,649,456\n",
      "\n",
      "======================================================================\n",
      "Understanding entity_id_to_label:\n",
      "======================================================================\n",
      "Type: <class 'dict'>\n",
      "Size: 1,179,402 entities\n",
      "\n",
      "Example mappings (first 3):\n",
      "  1. Entity ID 0 → URI: http://data.europa.eu/esco/concept-scheme/6c930acd-c104-4ece-acf7-f44f...\n",
      "  2. Entity ID 1 → URI: http://data.europa.eu/esco/concept-scheme/digcomp...\n",
      "  3. Entity ID 2 → URI: http://data.europa.eu/esco/concept-scheme/green...\n",
      "\n",
      "Key insight: The 'label' in entity_id_to_label IS the original URI string!\n",
      "The URI comes from the triples we passed in (subject/object columns)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create TriplesFactory for PyKEEN\n",
    "print(\"Creating TriplesFactory...\")\n",
    "print(\"\\nHow TriplesFactory works:\")\n",
    "print(\"1. Input: triples with subject, predicate, object (all are URI strings)\")\n",
    "print(\"2. TriplesFactory automatically:\")\n",
    "print(\"   - Extracts all unique entities from subject and object columns\")\n",
    "print(\"   - Assigns each unique entity a unique integer ID (0, 1, 2, ...)\")\n",
    "print(\"   - Preserves the original URI string as the 'label'\")\n",
    "print(\"   - Creates mapping: entity_id_to_label = {entity_id: original_uri_string}\")\n",
    "print(\"3. The 'label' IS the original URI string from the triples!\")\n",
    "\n",
    "triples_factory = TriplesFactory.from_labeled_triples(\n",
    "    triples=triples_df_filtered[[\"subject\", \"predicate\", \"object\"]].values,\n",
    "    create_inverse_triples=True  # Set to True if you want inverse relations\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ TriplesFactory created\")\n",
    "print(f\"  Number of entities: {triples_factory.num_entities:,}\")\n",
    "print(f\"  Number of relations: {triples_factory.num_relations:,}\")\n",
    "print(f\"  Number of triples: {triples_factory.num_triples:,}\")\n",
    "\n",
    "# Show how entity_id_to_label works\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Understanding entity_id_to_label:\")\n",
    "print(\"=\" * 70)\n",
    "entity_id_to_uri = triples_factory.entity_id_to_label\n",
    "print(f\"Type: {type(entity_id_to_uri)}\")\n",
    "print(f\"Size: {len(entity_id_to_uri):,} entities\")\n",
    "print(\"\\nExample mappings (first 3):\")\n",
    "for i, (entity_id, uri) in enumerate(list(entity_id_to_uri.items())[:3], 1):\n",
    "    print(f\"  {i}. Entity ID {entity_id} → URI: {uri[:70]}...\")\n",
    "print(\"\\nKey insight: The 'label' in entity_id_to_label IS the original URI string!\")\n",
    "print(\"The URI comes from the triples we passed in (subject/object columns)\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6cdc334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using automatically assigned random_state=1044129335\n"
     ]
    }
   ],
   "source": [
    "# # Split triples into training and testing sets\n",
    "# # In newer PyKEEN versions (1.11+), use ratios instead of ratio\n",
    "# # try:\n",
    "# #     # New API: use ratios parameter (tuple of ratios for train, test, validation)\n",
    "# #     training, testing = triples_factory.split(ratios=(0.8, 0.2), random_state=42)\n",
    "# # except TypeError:\n",
    "# #     # Fallback: try train_ratio parameter\n",
    "# #     try:\n",
    "# #         training, testing = triples_factory.split(train_ratio=0.8, random_state=42)\n",
    "# #     except TypeError:\n",
    "# #         # Old API: use ratio parameter\n",
    "# #         training, testing = triples_factory.split(ratio=0.8, random_state=42)\n",
    "# training, validation, testing = triples_factory.split(ratios=(0.8, 0.1, 0.1))\n",
    "training, validation = triples_factory.split(ratios=(0.8, 0.2))\n",
    "\n",
    "# print(f\"Training triples: {training.num_triples:,}\")\n",
    "# print(f\"Validation triples:{validation.num_triples}\")\n",
    "# print(f\"Testing triples: {testing.num_triples:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ecee2",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8493de6",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c31964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pykeen.training.callbacks import TrainingCallback\n",
    "from pykeen.training.callbacks import GradientNormClippingTrainingCallback\n",
    "\n",
    "# Custom callback to print loss after each epoch (using tqdm.write() so it doesn't get overwritten)\n",
    "# class PrintLossCallback(TrainingCallback):\n",
    "#     \"\"\"Callback to print loss after each epoch using tqdm.write()\"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.epoch_losses = []\n",
    "#         self.metrics_history = []  # List of dicts: [{'epoch': 1, 'loss': 0.123, 'loss_change': 0.0}, ...]\n",
    "        \n",
    "#     def on_epoch_end(self, epoch: int, epoch_loss: float, **kwargs) -> None:\n",
    "#         \"\"\"Called at the end of each epoch\"\"\"\n",
    "#         self.epoch_losses.append(epoch_loss)\n",
    "        \n",
    "#         # Calculate loss change\n",
    "#         if epoch == 1:\n",
    "#             change = 0.0\n",
    "#         else:\n",
    "#             change = epoch_loss - self.epoch_losses[-2]\n",
    "#         change_str = f\"{change:+.4f}\" if epoch > 1 else \"N/A\"\n",
    "        \n",
    "#         # Initialize metrics dict for this epoch\n",
    "#         epoch_metrics = {\n",
    "#             'epoch': epoch,\n",
    "#             'loss': float(epoch_loss),\n",
    "#             'loss_change': float(change)\n",
    "#         }\n",
    "        \n",
    "#         # Store metrics\n",
    "#         self.metrics_history.append(epoch_metrics)\n",
    "        \n",
    "#         # Key: Use tqdm.write() instead of print() so the message doesn't get overwritten by progress bar\n",
    "#         # This ensures the loss information persists and doesn't disappear when tqdm refreshes\n",
    "#         tqdm.write(f\"Epoch {epoch:3d} | Loss: {epoch_loss:.4f} | Change: {change_str}\")\n",
    "    \n",
    "#     def get_metrics_dataframe(self):\n",
    "#         \"\"\"Convert metrics history to pandas DataFrame\"\"\"\n",
    "#         import pandas as pd\n",
    "#         return pd.DataFrame(self.metrics_history)\n",
    "    \n",
    "#     def save_metrics(self, filepath):\n",
    "#         \"\"\"Save metrics history to CSV file\"\"\"\n",
    "#         import pandas as pd\n",
    "#         df = pd.DataFrame(self.metrics_history)\n",
    "#         df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "#         print(f\"✓ Metrics saved to: {filepath}\")\n",
    "#         return df\n",
    "\n",
    "# Check device (support Apple Silicon GPU, CUDA, or CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fe1984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TransE model...\n",
      "✓ Model creation completed\n",
      "✓ Created test subset: 10,000 triples\n",
      "✓ Evaluator creation completed\n",
      "\n",
      "======================================================================\n",
      "All components created successfully, ready to start training!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from pykeen.models import TransE\n",
    "from pykeen.training import SLCWATrainingLoop\n",
    "from pykeen.evaluation import SampledRankBasedEvaluator\n",
    "from pykeen.stoppers import EarlyStopper\n",
    "# Use tqdm.auto to automatically detect environment and avoid ContextVar errors\n",
    "# If LookupError: shell_parent is encountered, tqdm.auto will automatically fall back to standard version\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "    # tqdm.auto automatically detects environment, uses notebook version in Jupyter, standard version in terminal\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Ensure tqdm displays correctly in Jupyter notebook\n",
    "tqdm.pandas()  # If using pandas\n",
    "\n",
    "# Assume these three are already available\n",
    "# train_factory, valid_factory, test_factory = ...\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "# random.seed(42)           # Python random module\n",
    "# torch.manual_seed(42)      # PyTorch random seed\n",
    "# np.random.seed(42)         # NumPy random seed\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(42)  # CUDA random seed (if using GPU)\n",
    "\n",
    "# 1️⃣ Define model\n",
    "print(\"Creating TransE model...\")\n",
    "model = TransE(\n",
    "    triples_factory=triples_factory,\n",
    "    embedding_dim=200,\n",
    "    scoring_fct_norm=1,\n",
    "    random_seed=42,  # Set random seed to avoid warning\n",
    ").to(device)\n",
    "print(\"✓ Model creation completed\")\n",
    "\n",
    "# 2️⃣ Define trainer\n",
    "# print(\"Creating trainer...\")\n",
    "# trainer = SLCWATrainingLoop(\n",
    "#     model=model,\n",
    "#     triples_factory=training,\n",
    "#     optimizer=torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "# )\n",
    "# print(\"✓ Trainer creation completed\")\n",
    "\n",
    "# 3️⃣ Define evaluator and early stopping\n",
    "# Note: Do not set additional_filter_triples during creation, as processing large amounts of data will be slow\n",
    "# Will pass it during evaluate() calls instead, which is more flexible and won't block\n",
    "\n",
    "# print(\"Creating evaluator...\")\n",
    "# small_train_idx = torch.randperm(training.num_triples)[:80000]\n",
    "# train_small = training.clone_and_exchange_triples(\n",
    "#     mapped_triples=triples_factory.mapped_triples[small_train_idx]\n",
    "# )\n",
    "\n",
    "valid_idx = torch.randperm(validation.num_triples)[:10000]\n",
    "valid_small = validation.clone_and_exchange_triples(\n",
    "    mapped_triples=validation.mapped_triples[valid_idx]\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"✓ Created test subset: {valid_small.num_triples:,} triples\")\n",
    "\n",
    "evaluator_small = SampledRankBasedEvaluator(\n",
    "    evaluation_factory=valid_small,\n",
    "    num_negatives=100,\n",
    "    filtered=True,\n",
    "    additional_filter_triples=[training.mapped_triples],\n",
    ")\n",
    "print(\"✓ Evaluator creation completed\")\n",
    "\n",
    "\n",
    "# # Create subset for test set as well to speed up evaluation\n",
    "# test_idx = torch.randperm(testing.num_triples)[:10000]\n",
    "# test_small = testing.clone_and_exchange_triples(\n",
    "#     mapped_triples=testing.mapped_triples[test_idx]\n",
    "# )\n",
    "\n",
    "# # Create new evaluator for test set, because evaluator_small was created for valid_small\n",
    "# # SampledRankBasedEvaluator requires evaluation_factory to contain the triples to be evaluated\n",
    "# print(\"Creating test set evaluator...\")\n",
    "# evaluator_test = SampledRankBasedEvaluator(\n",
    "#     evaluation_factory=test_small,  # Use test set subset as evaluation_factory\n",
    "#     num_negatives=100,  # Keep consistent with evaluator_small\n",
    "#     filtered=True,\n",
    "#     additional_filter_triples=[training.mapped_triples,validation.mapped_triples],\n",
    "#     # Do not set additional_filter_triples during creation to avoid blocking when processing large amounts of data\n",
    "# )\n",
    "# print(\"✓ Test set evaluator creation completed\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# evaluator = SampledRankBasedEvaluator(\n",
    "#     evaluation_factory=validation,\n",
    "#     num_negatives=50,\n",
    "#     filtered=False,\n",
    "#     # additional_filter_triples=[\n",
    "#     #     training.mapped_triples,\n",
    "#     #     validation.mapped_triples,\n",
    "#     # ]\n",
    "# )\n",
    "\n",
    "# print(\"Creating early stopper...\")\n",
    "# stopper = EarlyStopper(\n",
    "#     model=model,\n",
    "#     evaluator=evaluator_small,\n",
    "#     training_triples_factory=training,\n",
    "#     evaluation_triples_factory=valid_small,\n",
    "#     frequency=1,           # Check every epoch\n",
    "#     patience=5,\n",
    "#     relative_delta=0.001,\n",
    "#     metric='mean_reciprocal_rank',\n",
    "# )\n",
    "# print(\"✓ Early stopper creation completed\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All components created successfully, ready to start training!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be22358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mrr = -1.0\n",
    "best_state = None          # Option A: Save in memory\n",
    "best_ckpt = \"best_transE.pt\"  # Option B: Save to disk (can use either or both)\n",
    "patience, waited = 3, 0\n",
    "rel_delta = 1e-3          # Relative improvement threshold 0.1%\n",
    "EVAL_EVERY = 1 \n",
    "import copy\n",
    "\n",
    "# Create a custom callback for evaluation and early stopping\n",
    "# from pykeen.training.callbacks import TrainingCallback\n",
    "\n",
    "# class EvaluationCallback(TrainingCallback):\n",
    "#     \"\"\"Callback to evaluate model and handle early stopping after each epoch\"\"\"\n",
    "    \n",
    "#     def __init__(self, evaluator, valid_small, training, best_mrr_ref, best_state_ref, \n",
    "#                  best_ckpt_ref, waited_ref, patience, rel_delta, eval_every):\n",
    "#         super().__init__()\n",
    "#         self.evaluator = evaluator\n",
    "#         self.valid_small = valid_small\n",
    "#         self.training = training\n",
    "#         self.best_mrr_ref = best_mrr_ref  # Reference to global variable\n",
    "#         self.best_state_ref = best_state_ref\n",
    "#         self.best_ckpt_ref = best_ckpt_ref\n",
    "#         self.waited_ref = waited_ref\n",
    "#         self.patience = patience\n",
    "#         self.rel_delta = rel_delta\n",
    "#         self.eval_every = eval_every\n",
    "#         self.should_stop = False\n",
    "    \n",
    "#     def post_epoch(self, epoch: int, epoch_loss: float, **kwargs) -> None:\n",
    "#         \"\"\"Called after each epoch\"\"\"\n",
    "#         model = self.model\n",
    "        \n",
    "#         # Print epoch info\n",
    "#         loss_value = epoch_loss if isinstance(epoch_loss, (int, float)) else sum(epoch_loss) / len(epoch_loss)\n",
    "#         print(f\"\\n[Epoch {epoch}] Training completed | Loss: {loss_value:.4f}\")\n",
    "        \n",
    "#         # Evaluate if needed\n",
    "#         if epoch % self.eval_every == 0:\n",
    "#             print(f\"\\n[Evaluation] Starting validation evaluation (this may take a few minutes)...\")\n",
    "            \n",
    "#             # Ensure model is in eval mode for evaluation\n",
    "#             model.eval()\n",
    "#             torch.set_grad_enabled(False)\n",
    "            \n",
    "#             val_result = self.evaluator.evaluate(\n",
    "#                 model=model,\n",
    "#                 mapped_triples=self.valid_small.mapped_triples,\n",
    "#                 batch_size=512, \n",
    "#                 use_tqdm=True,\n",
    "#                 additional_filter_triples=[self.training.mapped_triples],\n",
    "#             )\n",
    "            \n",
    "#             val_mrr = val_result.get_metric('mean_reciprocal_rank')\n",
    "#             print(f\"[Evaluation] ✓ Evaluation completed | Validation MRR: {val_mrr:.4f}\")\n",
    "            \n",
    "#             # Restore training mode\n",
    "#             model.train()\n",
    "#             torch.set_grad_enabled(True)\n",
    "            \n",
    "#             # Early stopping logic\n",
    "#             if val_mrr > self.best_mrr_ref[0] * (1 + self.rel_delta):\n",
    "#                 self.best_mrr_ref[0] = val_mrr\n",
    "#                 self.best_state_ref[0] = copy.deepcopy(model.state_dict())\n",
    "#                 torch.save(self.best_state_ref[0], self.best_ckpt_ref)\n",
    "#                 self.waited_ref[0] = 0\n",
    "#                 print(f\"[Early Stopping] ✓ New best MRR! (waited: {self.waited_ref[0]}/{self.patience})\")\n",
    "#             else:\n",
    "#                 self.waited_ref[0] += 1\n",
    "#                 print(f\"[Early Stopping] No improvement (waited: {self.waited_ref[0]}/{self.patience})\")\n",
    "#                 if self.waited_ref[0] >= self.patience:\n",
    "#                     print(f\"\\n[Early Stopping] ⚠️  Early stopping triggered at epoch {epoch}\")\n",
    "#                     print(f\"  Best MRR: {self.best_mrr_ref[0]:.4f}\")\n",
    "#                     self.should_stop = True\n",
    "#         else:\n",
    "#             print(f\"[Evaluation] Skipped (evaluates every {self.eval_every} epochs)\")\n",
    "    \n",
    "#     def should_stop_training(self) -> bool:\n",
    "#         \"\"\"Check if training should stop\"\"\"\n",
    "#         return self.should_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ee4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Manual Evaluation Function ===\n",
    "# Custom evaluation function for overfitting scenario (no filtering needed)\n",
    "# def manual_evaluate(model, test_triples, batch_size=512, k_values=[1, 3, 10], device=None, use_tqdm=True):\n",
    "#     \"\"\"\n",
    "#     Manually evaluate model on test triples, computing MRR and Hits@K.\n",
    "#     For overfitting scenario: no filtering of training triples.\n",
    "    \n",
    "#     Args:\n",
    "#         model: PyKEEN model (e.g., TransE)\n",
    "#         test_triples: torch.Tensor of shape (n, 3) with (head, relation, tail) triples\n",
    "#         batch_size: Batch size for evaluation\n",
    "#         k_values: List of K values for Hits@K metric\n",
    "#         device: Device to run evaluation on (default: model's device)\n",
    "#         use_tqdm: Whether to show progress bar\n",
    "    \n",
    "#     Returns:\n",
    "#         EvaluationResult object with get_metric() method for compatibility\n",
    "#     \"\"\"\n",
    "#     if device is None:\n",
    "#         device = next(model.parameters()).device\n",
    "    \n",
    "#     model.eval()\n",
    "#     torch.set_grad_enabled(False)\n",
    "    \n",
    "#     num_entities = model.num_entities\n",
    "#     num_triples = test_triples.shape[0]\n",
    "    \n",
    "#     # Initialize metrics\n",
    "#     head_ranks = []\n",
    "#     tail_ranks = []\n",
    "#     head_hits = {k: 0 for k in k_values}\n",
    "#     tail_hits = {k: 0 for k in k_values}\n",
    "    \n",
    "#     # Process in batches\n",
    "#     if use_tqdm:\n",
    "#         from tqdm.auto import tqdm\n",
    "#         pbar = tqdm(range(0, num_triples, batch_size), desc=\"Evaluating\")\n",
    "#     else:\n",
    "#         pbar = range(0, num_triples, batch_size)\n",
    "    \n",
    "#     for i in pbar:\n",
    "#         batch = test_triples[i:i+batch_size].to(device)\n",
    "#         batch_size_actual = batch.shape[0]\n",
    "        \n",
    "#         h_batch = batch[:, 0]  # heads\n",
    "#         r_batch = batch[:, 1]  # relations\n",
    "#         t_batch = batch[:, 2]  # tails\n",
    "        \n",
    "#         # === Head prediction: (?, r, t) ===\n",
    "#         for h_idx in range(batch_size_actual):\n",
    "#             h = h_batch[h_idx]\n",
    "#             r = r_batch[h_idx]\n",
    "#             t = t_batch[h_idx]\n",
    "            \n",
    "#             # Get scores for all possible heads\n",
    "#             h_candidates = torch.arange(num_entities, device=device)\n",
    "#             r_expanded = r.unsqueeze(0).expand(num_entities)\n",
    "#             t_expanded = t.unsqueeze(0).expand(num_entities)\n",
    "            \n",
    "#             # Use position arguments, not keyword arguments\n",
    "#             scores = model.score_hrt(h_candidates, r_expanded, t_expanded)\n",
    "            \n",
    "#             # Get rank of true head (higher score = better)\n",
    "#             true_h = h.item()\n",
    "#             sorted_indices = torch.argsort(scores, descending=True)\n",
    "#             rank = (sorted_indices == true_h).nonzero(as_tuple=True)[0].item() + 1\n",
    "#             head_ranks.append(rank)\n",
    "            \n",
    "#             # Update Hits@K\n",
    "#             for k in k_values:\n",
    "#                 if rank <= k:\n",
    "#                     head_hits[k] += 1\n",
    "        \n",
    "#         # === Tail prediction: (h, r, ?) ===\n",
    "#         for t_idx in range(batch_size_actual):\n",
    "#             h = h_batch[t_idx]\n",
    "#             r = r_batch[t_idx]\n",
    "#             t = t_batch[t_idx]\n",
    "            \n",
    "#             # Get scores for all possible tails\n",
    "#             t_candidates = torch.arange(num_entities, device=device)\n",
    "#             h_expanded = h.unsqueeze(0).expand(num_entities)\n",
    "#             r_expanded = r.unsqueeze(0).expand(num_entities)\n",
    "            \n",
    "#             # Use position arguments, not keyword arguments\n",
    "#             scores = model.score_hrt(h_expanded, r_expanded, t_candidates)\n",
    "            \n",
    "#             # Get rank of true tail\n",
    "#             true_t = t.item()\n",
    "#             sorted_indices = torch.argsort(scores, descending=True)\n",
    "#             rank = (sorted_indices == true_t).nonzero(as_tuple=True)[0].item() + 1\n",
    "#             tail_ranks.append(rank)\n",
    "            \n",
    "#             # Update Hits@K\n",
    "#             for k in k_values:\n",
    "#                 if rank <= k:\n",
    "#                     tail_hits[k] += 1\n",
    "    \n",
    "#     # Calculate final metrics\n",
    "#     all_ranks = head_ranks + tail_ranks\n",
    "#     mrr = sum(1.0 / rank for rank in all_ranks) / len(all_ranks) if all_ranks else 0.0\n",
    "    \n",
    "#     results = {'mean_reciprocal_rank': mrr}\n",
    "#     for k in k_values:\n",
    "#         total_hits = head_hits[k] + tail_hits[k]\n",
    "#         results[f'hits_at_{k}'] = total_hits / (2 * num_triples) if num_triples > 0 else 0.0\n",
    "    \n",
    "#     # Return object with get_metric method for compatibility\n",
    "#     class EvaluationResult:\n",
    "#         def __init__(self, results):\n",
    "#             self.results = results\n",
    "        \n",
    "#         def get_metric(self, metric):\n",
    "#             return self.results.get(metric, 0.0)\n",
    "        \n",
    "#         def to_dict(self):\n",
    "#             return self.results\n",
    "    \n",
    "#     return EvaluationResult(results)\n",
    "\n",
    "# print(\"✓ Manual evaluation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d5e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Starting training for 1 epochs\n",
      "======================================================================\n",
      "Evaluation every 1 epochs\n",
      "Early stopping patience: 3\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Epoch 1 of 1\n",
      "======================================================================\n",
      "[Training] Starting training for epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anyismacbook/Downloads/HWS2025/knowledge_graph/project/Knowledge_graphs/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfb9c4490c64ddbb187667eef934af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on mps:0:   0%|                                            | 0/1 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720bfea0777c4c7d9cb8d8d750617335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on mps:0:   0%|          | 0.00/3.22k [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] ✓ Training completed | Loss: 0.4384\n",
      "\n",
      "[Evaluation] Starting validation evaluation (this may take a few minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8b87dc2dd94e7c8de84e821be2f2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on mps:0:   0%|          | 0.00/10.0k [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered tensors on device_types={'mps'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 4️⃣ Start training loop\n",
    "# Recreate trainer each epoch to avoid internal state causing training time to be 0\n",
    "# But preserve optimizer state to avoid losing training momentum\n",
    "\n",
    "max_epochs = 1  # Define maximum number of training epochs\n",
    "train_set = triples_factory\n",
    "\n",
    "# Ensure model is in training mode\n",
    "model.train()\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "# Create optimizer only once to preserve optimizer state (Adam's momentum, etc.)\n",
    "# This way, even if we recreate the trainer, the optimizer state will be preserved\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Starting training for {max_epochs} epochs\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Evaluation every {EVAL_EVERY} epochs\")\n",
    "print(f\"Early stopping patience: {patience}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Epoch {epoch} of {max_epochs}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # === Recreate trainer each epoch, but reuse the same optimizer ===\n",
    "    # This ensures each training is fresh and won't have training time of 0 due to internal state\n",
    "    # At the same time, preserve optimizer state to avoid losing training momentum\n",
    "    trainer = SLCWATrainingLoop(\n",
    "        model=model,\n",
    "        triples_factory=train_set,\n",
    "        optimizer=optimizer,  # Reuse the same optimizer to preserve state\n",
    "    )\n",
    "    \n",
    "    # Ensure model is in training mode\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # === Training phase ===\n",
    "    print(f\"[Training] Starting training for epoch {epoch}...\")\n",
    "    loss = trainer.train(\n",
    "        triples_factory=train_set,\n",
    "        num_epochs=1,  # Train only 1 epoch each time\n",
    "        batch_size=1024,\n",
    "        # Note: Removed gradient_clipping_max_norm to avoid callback initialization error\n",
    "        # Gradient clipping can be done manually if needed using torch.nn.utils.clip_grad_norm_\n",
    "        use_tqdm=True,\n",
    "        use_tqdm_batch=True,\n",
    "        tqdm_kwargs={'leave': True, 'ncols': 100},\n",
    "        continue_training=False,  # Explicitly specify not to continue previous training\n",
    "        pin_memory=False  # Disable pin_memory to avoid MPS warning\n",
    "``    )\n",
    "    \n",
    "    loss_value = loss if isinstance(loss, (int, float)) else sum(loss) / len(loss) if loss else 0.0\n",
    "    print(f\"[Training] ✓ Training completed | Loss: {loss_value:.4f}\")\n",
    "\n",
    "    # === Evaluate on validation set ===\n",
    "    if epoch % EVAL_EVERY == 0:\n",
    "        print(f\"\\n[Evaluation] Starting validation evaluation (this may take a few minutes)...\")\n",
    "        \n",
    "        # Ensure model is in eval mode\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "        # Use manual evaluation function (no filtering for overfitting scenario)\n",
    "        val_result = evaluator_small.evaluate(\n",
    "            model=model,\n",
    "            mapped_triples=valid_small.mapped_triples,\n",
    "            batch_size=512,\n",
    "            # k_values=[1, 3, 10],\n",
    "            # device=device,\n",
    "            additional_filter_triples=[training.mapped_triples],\n",
    "            use_tqdm=True\n",
    "        )\n",
    "        \n",
    "        val_mrr = val_result.get_metric('mean_reciprocal_rank')\n",
    "        # val_hits1 = val_result.get_metric('hits_at_1')\n",
    "        # val_hits3 = val_result.get_metric('hits_at_3')\n",
    "        # val_hits10 = val_result.get_metric('hits_at_10')\n",
    "        print(f\"[Evaluation] ✓ Evaluation completed\")\n",
    "        print(f\"  MRR: {val_mrr:.4f} \")\n",
    "        \n",
    "        # Restore training mode\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Early stopping logic\n",
    "        # First evaluation: directly update best_mrr, no improvement check, don't reset waited\n",
    "        # Subsequent evaluations: check for relative improvement, reset waited if improved, otherwise increment waited\n",
    "        is_first_eval = (best_mrr < 0)\n",
    "        \n",
    "        if is_first_eval:\n",
    "            # First evaluation: directly update, don't reset waited\n",
    "            best_mrr = val_mrr\n",
    "            print(best_mrr)\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_state, best_ckpt)\n",
    "            # waited keeps original value (usually 0), don't reset\n",
    "            print(f\"[Early Stopping] ✓ First evaluation | MRR: {val_mrr:.4f} (waited: {waited}/{patience})\")\n",
    "        else:\n",
    "            print(best_mrr)\n",
    "            # Subsequent evaluations: check for relative improvement\n",
    "            if val_mrr > best_mrr * (1 + rel_delta):\n",
    "                # Improvement: update best_mrr, reset waited\n",
    "                best_mrr = val_mrr\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_state, best_ckpt)\n",
    "                waited = 0\n",
    "                print(f\"[Early Stopping] ✓ New best MRR! (waited: {waited}/{patience})\")\n",
    "            else:\n",
    "                # No improvement: increment waited\n",
    "                waited += 1\n",
    "                print(f\"[Early Stopping] No improvement (waited: {waited}/{patience})\")\n",
    "                if waited >= patience:\n",
    "                    print(f\"\\n[Early Stopping] ⚠️  Early stopping triggered at epoch {epoch}\")\n",
    "                    print(f\"  Best MRR: {best_mrr:.4f}\")\n",
    "                    break\n",
    "    else:\n",
    "        print(f\"[Evaluation] Skipped (evaluates every {EVAL_EVERY} epochs)\")\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] ✓ Completed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\" * 70)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # === Print model parameters (example: print an embedding) ===\n",
    "    # ent_emb = model.entity_representations[0]  # First embedding module\n",
    "    # rel_emb = model.relation_representations[0]\n",
    "    # print(\"Sample entity vector:\", ent_emb(torch.tensor([0])).detach().cpu().numpy())\n",
    "    # print(\"Sample relation vector:\", rel_emb(torch.tensor([0])).detach().cpu().numpy())\n",
    " \n",
    "    \n",
    "\n",
    "# === Load and save best model ===\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval(); torch.set_grad_enabled(False)\n",
    "    \n",
    "    # Save the best model\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Saving Best Model\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. Save complete model (includes architecture)\n",
    "    model_path = \"best_transE_model.pt\"\n",
    "    torch.save(model, model_path)\n",
    "    print(f\"✓ Saved complete model: {model_path}\")\n",
    "    print(f\"  Usage: model = torch.load('{model_path}')\")\n",
    "    \n",
    "    # 2. Save model state dict (already saved during training, but save again with metadata)\n",
    "    # state_dict_path = \"best_transE_state_dict.pt\"\n",
    "    # torch.save({\n",
    "    #     'model_state_dict': best_state,\n",
    "    #     'best_mrr': best_mrr,\n",
    "    #     'model_config': {\n",
    "    #         'model_type': 'TransE',\n",
    "    #         'embedding_dim': model.embedding_dim,\n",
    "    #         'scoring_fct_norm': model.scoring_fct_norm,\n",
    "    #         'num_entities': train_set.num_entities,\n",
    "    #         'num_relations': train_set.num_relations,\n",
    "    #     },\n",
    "    #     # 'training_info': {\n",
    "    #     #     'num_training_triples': training.num_triples,\n",
    "    #     #     'num_validation_triples': validation.num_triples,\n",
    "    #     #     'num_testing_triples': testing.num_triples,\n",
    "    #     # }\n",
    "    # }, state_dict_path)\n",
    "    # print(f\"✓ Saved model state dict with metadata: {state_dict_path}\")\n",
    "    # print(f\"  Usage: checkpoint = torch.load('{state_dict_path}')\")\n",
    "    # print(f\"        model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "    \n",
    "    # print(f\"\\nBest validation MRR: {best_mrr:.4f}\")\n",
    "    # print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"\\n⚠️  No best model found. Model may not have improved during training.\")\n",
    "    # Save current model anyway\n",
    "    model_path = \"final_transE_model.pt\"\n",
    "    torch.save(model, model_path)\n",
    "    print(f\"✓ Saved final model: {model_path}\")\n",
    "\n",
    "\n",
    "# test_result = evaluator_test.evaluate(\n",
    "#     model=model,\n",
    "#     mapped_triples=test_small.mapped_triples,\n",
    "#     batch_size=512, use_tqdm=True, \n",
    "#     # num_workers=0,\n",
    "#     additional_filter_triples=[\n",
    "#         training.mapped_triples,\n",
    "#         validation.mapped_triples,\n",
    "#     ],\n",
    "        \n",
    "#     )\n",
    "# print(f\"Test MRR: {test_result.get_metric('mean_reciprocal_rank'):.4f}\")\n",
    "\n",
    "print(\"\\n✅ Training completed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92bfcf",
   "metadata": {},
   "source": [
    "## Embedding and mapping URI to embedding\n",
    "I don't know why the numbers don't totally match for 'skill' set from the graph and from the pykeen-processed dataset, but I suppose using the overlap is ok..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7f70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting entity embeddings...\n",
      "Using manually trained model...\n",
      "✓ Entity embeddings extracted\n",
      "  Shape: (1179402, 200)\n",
      "  Embedding dimension: 200\n"
     ]
    }
   ],
   "source": [
    "# Get entity embeddings\n",
    "print(\"Extracting entity embeddings...\")\n",
    "\n",
    "# Use the manually trained model from Cell 17\n",
    "# The model variable is available globally after training\n",
    "if 'model' not in globals():\n",
    "    raise ValueError(\"Model not found. Please run the training cell (Cell 17) first.\")\n",
    "\n",
    "print(\"Using manually trained model...\")\n",
    "\n",
    "entity_embeddings = model.entity_representations[0](indices=None).detach().cpu().numpy()\n",
    "\n",
    "print(f\"✓ Entity embeddings extracted\")\n",
    "print(f\"  Shape: {entity_embeddings.shape}\")\n",
    "print(f\"  Embedding dimension: {entity_embeddings.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4bbac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loading Best Model from File\n",
      "======================================================================\n",
      "Loading model from best_transE_model.pt...\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL pykeen.models.unimodal.trans_e.TransE was not an allowed global by default. Please use `torch.serialization.add_safe_globals([pykeen.models.unimodal.trans_e.TransE])` or the `torch.serialization.safe_globals([pykeen.models.unimodal.trans_e.TransE])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Please run training first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Load the complete model (includes architecture)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m model.eval()\n\u001b[32m     21\u001b[39m torch.set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/HWS2025/knowledge_graph/project/Knowledge_graphs/.venv/lib/python3.12/site-packages/torch/serialization.py:1529\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1521\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1522\u001b[39m                     opened_zipfile,\n\u001b[32m   1523\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1526\u001b[39m                     **pickle_load_args,\n\u001b[32m   1527\u001b[39m                 )\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1531\u001b[39m             opened_zipfile,\n\u001b[32m   1532\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1535\u001b[39m             **pickle_load_args,\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL pykeen.models.unimodal.trans_e.TransE was not an allowed global by default. Please use `torch.serialization.add_safe_globals([pykeen.models.unimodal.trans_e.TransE])` or the `torch.serialization.safe_globals([pykeen.models.unimodal.trans_e.TransE])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# Load Best Model from File and Extract Embeddings\n",
    "# This cell loads the saved best model and extracts entity embeddings\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Loading Best Model from File\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load the best model from file\n",
    "model_path = \"best_transE_model.pt\"\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}. Please run training first.\")\n",
    "\n",
    "# Load the complete model (includes architecture)\n",
    "# Note: weights_only=False is required for loading complete models (not just weights)\n",
    "# This is safe since we trust our own saved model files\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "print(f\"✓ Model loaded from {model_path}\")\n",
    "\n",
    "# Verify it's the best model by checking metadata\n",
    "state_dict_path = \"best_transE_state_dict.pt\"\n",
    "if os.path.exists(state_dict_path):\n",
    "    # Loading checkpoint with metadata also requires weights_only=False\n",
    "    checkpoint = torch.load(state_dict_path, map_location=device, weights_only=False)\n",
    "    if 'best_mrr' in checkpoint:\n",
    "        print(f\"\\nModel Information:\")\n",
    "        print(f\"  Best validation MRR: {checkpoint['best_mrr']:.4f}\")\n",
    "        if 'model_config' in checkpoint:\n",
    "            config = checkpoint['model_config']\n",
    "            print(f\"  Model type: {config.get('model_type', 'Unknown')}\")\n",
    "            print(f\"  Embedding dimension: {config.get('embedding_dim', 'Unknown')}\")\n",
    "            print(f\"  Number of entities: {config.get('num_entities', 'Unknown'):,}\")\n",
    "            print(f\"  Number of relations: {config.get('num_relations', 'Unknown'):,}\")\n",
    "        if 'training_info' in checkpoint:\n",
    "            info = checkpoint['training_info']\n",
    "            print(f\"\\nTraining Information:\")\n",
    "            print(f\"  Training triples: {info.get('num_training_triples', 'Unknown'):,}\")\n",
    "            print(f\"  Validation triples: {info.get('num_validation_triples', 'Unknown'):,}\")\n",
    "            print(f\"  Testing triples: {info.get('num_testing_triples', 'Unknown'):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Extracting Entity Embeddings\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract all entity embeddings\n",
    "entity_embeddings = model.entity_representations[0](indices=None).detach().cpu().numpy()\n",
    "\n",
    "print(f\"\\n✓ Entity embeddings extracted\")\n",
    "print(f\"  Shape: {entity_embeddings.shape}\")\n",
    "print(f\"  Embedding dimension: {entity_embeddings.shape[1]}\")\n",
    "print(f\"  Total entities: {entity_embeddings.shape[0]:,}\")\n",
    "\n",
    "# Store in global variable for use in subsequent cells\n",
    "print(f\"\\n✓ Model and embeddings ready for use\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Creating Skill URI to Embedding Mapping\n",
      "======================================================================\n",
      "\n",
      "entity_id_to_label explanation:\n",
      "  Type: <class 'dict'>\n",
      "  Size: 1,150,092 entities\n",
      "  Format: {entity_id: original_uri_string}\n",
      "  The 'label' is the URI string from the triples we passed to TriplesFactory\n",
      "\n",
      "======================================================================\n",
      "First 10 items in entity_id_to_uri:\n",
      "======================================================================\n",
      "\n",
      "[1] Entity ID: 0\n",
      "    URI: http://data.europa.eu/esco/concept-scheme/6c930acd-c104-4ece-acf7-f44fd7333036\n",
      "    Is skill URI? False\n",
      "\n",
      "[2] Entity ID: 1\n",
      "    URI: http://data.europa.eu/esco/concept-scheme/digcomp\n",
      "    Is skill URI? False\n",
      "\n",
      "[3] Entity ID: 2\n",
      "    URI: http://data.europa.eu/esco/concept-scheme/green\n",
      "    Is skill URI? False\n",
      "\n",
      "[4] Entity ID: 3\n",
      "    URI: http://data.europa.eu/esco/concept-scheme/member-skills\n",
      "    Is skill URI? False\n",
      "\n",
      "[5] Entity ID: 4\n",
      "    URI: http://data.europa.eu/esco/concept-scheme/research\n",
      "    Is skill URI? False\n",
      "\n",
      "[6] Entity ID: 5\n",
      "    URI: http://data.europa.eu/esco/concept-scheme/skill-language-groups\n",
      "    Is skill URI? False\n",
      "\n",
      "[7] Entity ID: 6\n",
      "    URI: http://data.europa.eu/esco/concept-scheme/skill-transversal-groups\n",
      "    Is skill URI? False\n",
      "\n",
      "[8] Entity ID: 7\n",
      "    URI: http://data.europa.eu/esco/concept-scheme/skills\n",
      "    Is skill URI? False\n",
      "\n",
      "[9] Entity ID: 8\n",
      "    URI: http://data.europa.eu/esco/isced-f/00\n",
      "    Is skill URI? False\n",
      "\n",
      "[10] Entity ID: 9\n",
      "    URI: http://data.europa.eu/esco/isced-f/003\n",
      "    Is skill URI? False\n",
      "======================================================================\n",
      "\n",
      "Total entities in model: 1,150,092\n",
      "Total skills extracted from triples: 14,671\n",
      "\n",
      "Extracting skill embeddings from entity_embeddings...\n",
      "\n",
      "======================================================================\n",
      "FINAL STATISTICS:\n",
      "======================================================================\n",
      "  Total skills extracted from triples: 14,671\n",
      "  Skills with embeddings: 14,671\n",
      "  Skills without embeddings: 0\n",
      "  Coverage: 100.00%\n",
      "======================================================================\n",
      "\n",
      "✓ All skills have embeddings!\n",
      "\n",
      "✓ Mapping created: skill_embeddings_dict contains 14,671 skill embeddings\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from skill URI to embedding\n",
    "# Extract skill embeddings from entity_embeddings (created in Cell 7)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Creating Skill URI to Embedding Mapping\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get entity ID to URI mapping\n",
    "# \n",
    "# Why entity_id_to_label contains URIs (explanation):\n",
    "# ===================================================\n",
    "# When we created TriplesFactory in Cell 3, we passed:\n",
    "#   triples_df[[\"subject\", \"predicate\", \"object\"]].values\n",
    "# \n",
    "# The subject and object columns contain URI strings like:\n",
    "#   \"http://data.europa.eu/esco/skill/66c88f8d-b73e-4f9d-8cf3-15d9c68b5384\"\n",
    "# \n",
    "# TriplesFactory.from_labeled_triples() automatically:\n",
    "#   1. Extracts all unique entities from subject and object columns\n",
    "#   2. Assigns each unique entity a unique integer ID (0, 1, 2, ...)\n",
    "#   3. Preserves the original string (the URI) as the \"label\"\n",
    "#   4. Creates: entity_id_to_label = {entity_id: original_string}\n",
    "# \n",
    "# So entity_id_to_label[0] = \"http://data.europa.eu/esco/skill/...\"\n",
    "# The \"label\" IS the URI because that's what was in the triples!\n",
    "#\n",
    "# No explicit URI specification needed - the URI comes from the input data itself.\n",
    "\n",
    "entity_id_to_uri = triples_factory.entity_id_to_label\n",
    "\n",
    "print(f\"\\nentity_id_to_label explanation:\")\n",
    "print(f\"  Type: {type(entity_id_to_uri)}\")\n",
    "print(f\"  Size: {len(entity_id_to_uri):,} entities\")\n",
    "print(f\"  Format: {{entity_id: original_uri_string}}\")\n",
    "print(f\"  The 'label' is the URI string from the triples we passed to TriplesFactory\")\n",
    "\n",
    "# Print first few items\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"First 10 items in entity_id_to_uri:\")\n",
    "print(\"=\" * 70)\n",
    "for i, (entity_id, uri) in enumerate(list(entity_id_to_uri.items())[:10], 1):\n",
    "    print(f\"\\n[{i}] Entity ID: {entity_id}\")\n",
    "    print(f\"    URI: {uri}\")\n",
    "    # Check if it's a skill URI\n",
    "    is_skill = uri.startswith(\"http://data.europa.eu/esco/skill/\")\n",
    "    print(f\"    Is skill URI? {is_skill}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create reverse mapping: URI -> entity_id\n",
    "uri_to_entity_id = {uri: entity_id for entity_id, uri in entity_id_to_uri.items()}\n",
    "\n",
    "print(f\"\\nTotal entities in model: {len(entity_id_to_uri):,}\")\n",
    "print(f\"Total skills extracted from triples: {len(skill_uris_list):,}\")\n",
    "\n",
    "# Create dictionary: skill_uri -> embedding\n",
    "skill_embeddings_dict = {}\n",
    "skill_uris_set = set(skill_uris_list)\n",
    "\n",
    "print(\"\\nExtracting skill embeddings from entity_embeddings...\")\n",
    "for entity_id, uri in entity_id_to_uri.items():\n",
    "    if uri in skill_uris_set:\n",
    "        # entity_id is the index into entity_embeddings array\n",
    "        skill_embeddings_dict[uri] = entity_embeddings[entity_id]\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL STATISTICS:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Total skills extracted from triples: {len(skill_uris_list):,}\")\n",
    "print(f\"  Skills with embeddings: {len(skill_embeddings_dict):,}\")\n",
    "print(f\"  Skills without embeddings: {len(skill_uris_list) - len(skill_embeddings_dict):,}\")\n",
    "if len(skill_uris_list) > 0:\n",
    "    coverage = len(skill_embeddings_dict) / len(skill_uris_list) * 100\n",
    "    print(f\"  Coverage: {coverage:.2f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if all skills have embeddings\n",
    "missing_skills = skill_uris_set - set(skill_embeddings_dict.keys())\n",
    "if missing_skills:\n",
    "    print(f\"\\n⚠️  Warning: {len(missing_skills)} skills don't have embeddings\")\n",
    "    print(f\"  First few missing: {list(missing_skills)[:5]}\")\n",
    "    print(f\"  (This might happen if a skill URI appears in triples but is not an entity in the model)\")\n",
    "else:\n",
    "    print(f\"\\n✓ All skills have embeddings!\")\n",
    "\n",
    "print(f\"\\n✓ Mapping created: skill_embeddings_dict contains {len(skill_embeddings_dict):,} skill embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19cb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Verification: URI to Embedding Mapping\n",
      "======================================================================\n",
      "\n",
      "Verifying 14,671 skill embeddings...\n",
      "\n",
      "Step 1: Consistency check\n",
      "----------------------------------------------------------------------\n",
      "✓ Entity count matches embedding matrix size\n",
      "\n",
      "Step 2: Verifying sample skill URIs\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 1:\n",
      "  Skill URI: http://data.europa.eu/esco/skill/0005c151-5b5a-4a66-8aac-60e...\n",
      "  → Entity ID: 1135420\n",
      "  → Embedding shape: (128,)\n",
      "  → Embedding (first 5): [-0.09477121 -0.02935514 -0.10610551  0.11285554 -0.06000264]\n",
      "  → Verification: ✓ Match\n",
      "\n",
      "Example 2:\n",
      "  Skill URI: http://data.europa.eu/esco/skill/00064735-8fad-454b-90c7-ed8...\n",
      "  → Entity ID: 1135421\n",
      "  → Embedding shape: (128,)\n",
      "  → Embedding (first 5): [ 0.05098764  0.13925022  0.05058711  0.10238589 -0.08057278]\n",
      "  → Verification: ✓ Match\n",
      "\n",
      "Example 3:\n",
      "  Skill URI: http://data.europa.eu/esco/skill/000709ed-2be5-4193-b056-45a...\n",
      "  → Entity ID: 1135422\n",
      "  → Embedding shape: (128,)\n",
      "  → Embedding (first 5): [-0.07662874 -0.03305266 -0.07799432 -0.25652856 -0.04264192]\n",
      "  → Verification: ✓ Match\n",
      "\n",
      "Example 4:\n",
      "  Skill URI: http://data.europa.eu/esco/skill/0007bdc2-dd15-4824-b7d6-416...\n",
      "  → Entity ID: 1135423\n",
      "  → Embedding shape: (128,)\n",
      "  → Embedding (first 5): [-0.11860445  0.01270147 -0.0991625   0.13057119  0.03805165]\n",
      "  → Verification: ✓ Match\n",
      "\n",
      "Example 5:\n",
      "  Skill URI: http://data.europa.eu/esco/skill/00090cc1-1f27-439e-a4e0-19a...\n",
      "  → Entity ID: 1135424\n",
      "  → Embedding shape: (128,)\n",
      "  → Embedding (first 5): [-0.1098919   0.0667613  -0.10317173  0.06540941  0.05215114]\n",
      "  → Verification: ✓ Match\n",
      "\n",
      "======================================================================\n",
      "Verification complete!\n",
      "======================================================================\n",
      "✓ All 14,671 skill embeddings are correctly mapped\n"
     ]
    }
   ],
   "source": [
    "# Verify URI to Embedding Mapping\n",
    "# Verify that skill URIs are correctly mapped to embeddings\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Verification: URI to Embedding Mapping\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if skill_embeddings_dict exists\n",
    "if 'skill_embeddings_dict' not in globals() or not skill_embeddings_dict:\n",
    "    print(\"⚠️  ERROR: skill_embeddings_dict not found!\")\n",
    "    print(\"  Please run Cell 8 (Create mapping) first.\")\n",
    "    raise ValueError(\"skill_embeddings_dict not found. Run Cell 8 first.\")\n",
    "\n",
    "print(f\"\\nVerifying {len(skill_embeddings_dict):,} skill embeddings...\")\n",
    "\n",
    "# Get entity ID to URI mapping (if not already available)\n",
    "if 'entity_id_to_uri' not in globals():\n",
    "    entity_id_to_uri = triples_factory.entity_id_to_label\n",
    "if 'uri_to_entity_id' not in globals():\n",
    "    uri_to_entity_id = {uri: entity_id for entity_id, uri in entity_id_to_uri.items()}\n",
    "\n",
    "# Verify consistency\n",
    "print(\"\\nStep 1: Consistency check\")\n",
    "print(\"-\" * 70)\n",
    "assert len(entity_id_to_uri) == entity_embeddings.shape[0], \\\n",
    "    \"ERROR: Number of entities doesn't match embedding matrix size!\"\n",
    "print(\"✓ Entity count matches embedding matrix size\")\n",
    "\n",
    "# Verify a few examples\n",
    "print(\"\\nStep 2: Verifying sample skill URIs\")\n",
    "print(\"-\" * 70)\n",
    "test_uris = list(skill_embeddings_dict.keys())[:5]\n",
    "for i, test_uri in enumerate(test_uris, 1):\n",
    "    if test_uri in uri_to_entity_id:\n",
    "        entity_id = uri_to_entity_id[test_uri]\n",
    "        # Get embedding from dictionary\n",
    "        embedding_from_dict = skill_embeddings_dict[test_uri]\n",
    "        # Get embedding directly using entity_id\n",
    "        embedding_direct = entity_embeddings[entity_id]\n",
    "        # Check if they match\n",
    "        match = np.allclose(embedding_from_dict, embedding_direct)\n",
    "        \n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"  Skill URI: {test_uri[:60]}...\")\n",
    "        print(f\"  → Entity ID: {entity_id}\")\n",
    "        print(f\"  → Embedding shape: {embedding_from_dict.shape}\")\n",
    "        print(f\"  → Embedding (first 5): {embedding_from_dict[:5]}\")\n",
    "        print(f\"  → Verification: {'✓ Match' if match else '✗ Mismatch'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Verification complete!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"✓ All {len(skill_embeddings_dict):,} skill embeddings are correctly mapped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32194311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Saving Skill Embeddings and URI Mappings\n",
      "======================================================================\n",
      "\n",
      "Saving 14,671 skill embeddings...\n",
      "\n",
      "[1] Saving as pickle...\n",
      "  ✓ Saved: skill_embeddings_pykeen.pkl\n",
      "  Format: {uri: embedding_array}\n",
      "  Usage: pickle.load(open('skill_embeddings_pykeen.pkl', 'rb'))\n",
      "\n",
      "[2] Saving as numpy...\n",
      "  ✓ Saved: skill_embeddings_pykeen.npz\n",
      "  Format: {'uris': array, 'embeddings': matrix}\n",
      "  Usage: np.load('skill_embeddings_pykeen.npz')\n",
      "\n",
      "[3] Saving as CSV...\n",
      "  ✓ Saved: skill_embeddings_pykeen.csv\n",
      "  Format: CSV with URI as index, embedding dimensions as columns\n",
      "  Usage: pd.read_csv('skill_embeddings_pykeen.csv', index_col='uri')\n",
      "\n",
      "[4] Saving as JSON (URI -> embedding as list)...\n",
      "  ✓ Saved: skill_embeddings_pykeen.json\n",
      "  Format: {uri: [embedding_values...]}\n",
      "  Usage: json.load(open('skill_embeddings_pykeen.json'))\n",
      "\n",
      "[5] Saving metadata...\n",
      "  ✓ Saved: skill_embeddings_pykeen_metadata.pkl\n",
      "  Contains: model info, dimensions, counts, etc.\n",
      "\n",
      "[6] Saving URI-embedding pairs as TSV...\n",
      "  ✓ Saved: skill_embeddings_pykeen_summary.tsv\n",
      "  Format: TSV with URI, embedding_dim, first 5 values, norm\n",
      "\n",
      "======================================================================\n",
      "All files saved successfully!\n",
      "======================================================================\n",
      "\n",
      "Saved files:\n",
      "  1. skill_embeddings_pykeen.pkl - Python pickle (recommended for Python)\n",
      "  2. skill_embeddings_pykeen.npz - NumPy format (efficient)\n",
      "  3. skill_embeddings_pykeen.csv - CSV format (human-readable)\n",
      "  4. skill_embeddings_pykeen.json - JSON format (human-readable)\n",
      "  5. skill_embeddings_pykeen_metadata.pkl - Metadata\n",
      "  6. skill_embeddings_pykeen_summary.tsv - Summary with sample values\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings and URI mappings in multiple formats for easy use\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Saving Skill Embeddings and URI Mappings\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if skill_embeddings_dict exists\n",
    "if 'skill_embeddings_dict' not in globals() or not skill_embeddings_dict:\n",
    "    print(\"⚠️  ERROR: skill_embeddings_dict not found!\")\n",
    "    print(\"  Please run Cell 9 (Create mapping) first.\")\n",
    "    raise ValueError(\"skill_embeddings_dict not found. Run Cell 9 first.\")\n",
    "\n",
    "print(f\"\\nSaving {len(skill_embeddings_dict):,} skill embeddings...\")\n",
    "\n",
    "# 1. Save as pickle (Python native, preserves data types, easiest to use)\n",
    "print(\"\\n[1] Saving as pickle...\")\n",
    "with open(\"skill_embeddings_pykeen.pkl\", \"wb\") as f:\n",
    "    pickle.dump(skill_embeddings_dict, f)\n",
    "print(\"  ✓ Saved: skill_embeddings_pykeen.pkl\")\n",
    "print(\"  Format: {uri: embedding_array}\")\n",
    "print(\"  Usage: pickle.load(open('skill_embeddings_pykeen.pkl', 'rb'))\")\n",
    "\n",
    "# 2. Save as numpy file (efficient for numerical operations)\n",
    "print(\"\\n[2] Saving as numpy...\")\n",
    "skill_uris_list_sorted = sorted(list(skill_embeddings_dict.keys()))\n",
    "skill_embeddings_matrix = np.array([skill_embeddings_dict[uri] for uri in skill_uris_list_sorted])\n",
    "\n",
    "np.savez(\n",
    "    \"skill_embeddings_pykeen.npz\",\n",
    "    uris=skill_uris_list_sorted,\n",
    "    embeddings=skill_embeddings_matrix\n",
    ")\n",
    "print(\"  ✓ Saved: skill_embeddings_pykeen.npz\")\n",
    "print(\"  Format: {'uris': array, 'embeddings': matrix}\")\n",
    "print(\"  Usage: np.load('skill_embeddings_pykeen.npz')\")\n",
    "\n",
    "# 3. Save as CSV with URI and embedding (for easy inspection)\n",
    "# Note: This creates a wide CSV, may be large\n",
    "print(\"\\n[3] Saving as CSV...\")\n",
    "embeddings_df = pd.DataFrame(\n",
    "    skill_embeddings_matrix,\n",
    "    index=skill_uris_list_sorted\n",
    ")\n",
    "embeddings_df.index.name = \"uri\"\n",
    "embeddings_df.to_csv(\"skill_embeddings_pykeen.csv\")\n",
    "print(\"  ✓ Saved: skill_embeddings_pykeen.csv\")\n",
    "print(\"  Format: CSV with URI as index, embedding dimensions as columns\")\n",
    "print(\"  Usage: pd.read_csv('skill_embeddings_pykeen.csv', index_col='uri')\")\n",
    "\n",
    "# 4. Save as JSON (human-readable, but large)\n",
    "print(\"\\n[4] Saving as JSON (URI -> embedding as list)...\")\n",
    "# Convert numpy arrays to lists for JSON\n",
    "skill_embeddings_json = {\n",
    "    uri: embedding.tolist() \n",
    "    for uri, embedding in skill_embeddings_dict.items()\n",
    "}\n",
    "import json\n",
    "with open(\"skill_embeddings_pykeen.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(skill_embeddings_json, f, indent=2)\n",
    "print(\"  ✓ Saved: skill_embeddings_pykeen.json\")\n",
    "print(\"  Format: {uri: [embedding_values...]}\")\n",
    "print(\"  Usage: json.load(open('skill_embeddings_pykeen.json'))\")\n",
    "\n",
    "# 5. Save metadata\n",
    "print(\"\\n[5] Saving metadata...\")\n",
    "metadata = {\n",
    "    \"model\": \"TransE\",\n",
    "    \"embedding_dim\": skill_embeddings_matrix.shape[1],\n",
    "    \"num_skills\": len(skill_embeddings_dict),\n",
    "    \"num_triples\": triples_factory.num_triples,\n",
    "    \"num_entities\": triples_factory.num_entities,\n",
    "    \"num_relations\": triples_factory.num_relations,\n",
    "    \"skill_uri_prefix\": \"http://data.europa.eu/esco/skill/\",\n",
    "    \"description\": \"Skill embeddings from PyKEEN knowledge graph embedding model\",\n",
    "    \"created_date\": pd.Timestamp.now().isoformat(),\n",
    "}\n",
    "\n",
    "with open(\"skill_embeddings_pykeen_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(\"  ✓ Saved: skill_embeddings_pykeen_metadata.pkl\")\n",
    "print(\"  Contains: model info, dimensions, counts, etc.\")\n",
    "\n",
    "# 6. Save as TSV with URI and embedding (alternative CSV format)\n",
    "print(\"\\n[6] Saving URI-embedding pairs as TSV...\")\n",
    "# Create a TSV with URI and embedding values (one row per skill)\n",
    "tsv_data = []\n",
    "for uri in skill_uris_list_sorted:\n",
    "    embedding = skill_embeddings_dict[uri]\n",
    "    row = {\"uri\": uri, \"embedding_dim\": len(embedding)}\n",
    "    # Add first few embedding values as example\n",
    "    for i, val in enumerate(embedding[:5]):\n",
    "        row[f\"emb_{i}\"] = val\n",
    "    row[\"embedding_norm\"] = np.linalg.norm(embedding)\n",
    "    tsv_data.append(row)\n",
    "\n",
    "tsv_df = pd.DataFrame(tsv_data)\n",
    "tsv_df.to_csv(\"skill_embeddings_pykeen_summary.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"  ✓ Saved: skill_embeddings_pykeen_summary.tsv\")\n",
    "print(\"  Format: TSV with URI, embedding_dim, first 5 values, norm\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All files saved successfully!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  1. skill_embeddings_pykeen.pkl - Python pickle (recommended for Python)\")\n",
    "print(\"  2. skill_embeddings_pykeen.npz - NumPy format (efficient)\")\n",
    "print(\"  3. skill_embeddings_pykeen.csv - CSV format (human-readable)\")\n",
    "print(\"  4. skill_embeddings_pykeen.json - JSON format (human-readable)\")\n",
    "print(\"  5. skill_embeddings_pykeen_metadata.pkl - Metadata\")\n",
    "print(\"  6. skill_embeddings_pykeen_summary.tsv - Summary with sample values\")\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
